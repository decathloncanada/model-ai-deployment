{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "QqGwDkZKIGX1",
        "colab_type": "code",
        "outputId": "f2af332b-2dfa-40f7-fddb-176e12066faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        }
      },
      "cell_type": "code",
      "source": [
        "#example to train an mnist model\n",
        "from tensorflow.python import keras\n",
        "from tensorflow.python.keras.datasets import mnist\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 12s 193us/sample - loss: 0.2628 - acc: 0.9176 - val_loss: 0.0548 - val_acc: 0.9832\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0851 - acc: 0.9749 - val_loss: 0.0395 - val_acc: 0.9868\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0651 - acc: 0.9806 - val_loss: 0.0340 - val_acc: 0.9886\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 9s 145us/sample - loss: 0.0553 - acc: 0.9837 - val_loss: 0.0326 - val_acc: 0.9893\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0463 - acc: 0.9863 - val_loss: 0.0290 - val_acc: 0.9902\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0412 - acc: 0.9873 - val_loss: 0.0300 - val_acc: 0.9902\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0366 - acc: 0.9884 - val_loss: 0.0297 - val_acc: 0.9907\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0341 - acc: 0.9900 - val_loss: 0.0242 - val_acc: 0.9914\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 9s 144us/sample - loss: 0.0305 - acc: 0.9908 - val_loss: 0.0286 - val_acc: 0.9909\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 9s 144us/sample - loss: 0.0291 - acc: 0.9908 - val_loss: 0.0271 - val_acc: 0.9914\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 9s 144us/sample - loss: 0.0274 - acc: 0.9914 - val_loss: 0.0277 - val_acc: 0.9918\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 9s 144us/sample - loss: 0.0269 - acc: 0.9918 - val_loss: 0.0280 - val_acc: 0.9916\n",
            "Test loss: 0.028042900659284668\n",
            "Test accuracy: 0.9916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QFQ_Aso7qphW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "a06f6d6e-1b8e-4439-ba38-d9823f5bd527"
      },
      "cell_type": "code",
      "source": [
        "#export the model\n",
        "import tensorflow as tf\n",
        "export_path = \"./image_classifier/\"\n",
        "saved_model_path = tf.contrib.saved_model.save_keras_model(model, export_path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adadelta object at 0x7f70513114e0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
            "WARNING:tensorflow:Model was compiled with an optimizer, but the optimizer is not from `tf.train` (e.g. `tf.train.AdagradOptimizer`). Only the serving graph was exported. The train and evaluate graphs were not added to the SavedModel.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: ./image_classifier/1553111875/saved_model.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hYZBAdUQInCb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "63a4334f-d264-41eb-9eaf-96f5667ee897"
      },
      "cell_type": "code",
      "source": [
        "#save the model to the drive\n",
        "!zip -r image_classifier.zip image_classifier\n",
        "from google.colab import files\n",
        "files.download('image_classifier.zip')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image_classifier  sample_data\n",
            "  adding: image_classifier/ (stored 0%)\n",
            "  adding: image_classifier/1553111875/ (stored 0%)\n",
            "  adding: image_classifier/1553111875/assets/ (stored 0%)\n",
            "  adding: image_classifier/1553111875/assets/saved_model.json (deflated 82%)\n",
            "  adding: image_classifier/1553111875/variables/ (stored 0%)\n",
            "  adding: image_classifier/1553111875/variables/variables.data-00000-of-00001 (deflated 8%)\n",
            "  adding: image_classifier/1553111875/variables/checkpoint (deflated 37%)\n",
            "  adding: image_classifier/1553111875/variables/variables.index (deflated 56%)\n",
            "  adding: image_classifier/1553111875/saved_model.pb (deflated 85%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}